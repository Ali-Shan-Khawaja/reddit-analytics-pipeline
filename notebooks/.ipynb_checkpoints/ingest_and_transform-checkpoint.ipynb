{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a7a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_unixtime, to_date, hour, dayofmonth, month, year, weekofyear,lit,\n",
    "    date_format, minute, second, dense_rank, monotonically_increasing_id, max as spark_max\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198b1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit_ETL\") \\\n",
    "    .config(\"spark.hadoop.io.nativeio.enabled\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0334e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "input_folder = \"D:/Portfolio/reddit-analytics-pipeline/data/partitions\"\n",
    "dw_folder = \"D:/Portfolio/reddit-analytics-pipeline/data/DW_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3052fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure DW cache directory exists\n",
    "os.makedirs(dw_folder, exist_ok=True)\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreMissingFiles\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "366ab2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_files_in_directory(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        for item in os.listdir(folder_path):\n",
    "            item_path = os.path.join(folder_path, item)\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.remove(item_path)\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273570c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_folder_empty_of_parquet(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"Folder does not exist.\")\n",
    "        return False  # or raise Exception if needed\n",
    "\n",
    "    parquet_files = [f for f in os.listdir(folder_path) if f.endswith(\".parquet\")]\n",
    "    if not parquet_files:\n",
    "        print(\"Folder exists but contains no Parquet files.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"Folder has Parquet files.\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a35c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_new_csvs(input_path):\n",
    "    return spark.read.option(\"header\", True).csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27740e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_prepare(df):\n",
    "    return df.select(\n",
    "        col(\"id\").alias(\"comment_id\"),\n",
    "        col(\"author\"),\n",
    "        col(\"subreddit\"),\n",
    "        col(\"subreddit_id\"),\n",
    "        col(\"created_utc\"),\n",
    "        col(\"body\"),\n",
    "        col(\"score\"),\n",
    "        col(\"controversiality\"),\n",
    "        col(\"gilded\"),\n",
    "        col(\"ups\"),\n",
    "        col(\"downs\")\n",
    "    ).dropna(subset=[\"author\", \"subreddit\", \"created_utc\", \"comment_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "925a45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dim_user(df, dw_path):\n",
    "    spark.catalog.clearCache()\n",
    "    user_path = os.path.join(dw_path, \"dim_user\")\n",
    "\n",
    "    # Clean input: remove nulls and drop duplicates\n",
    "    new_users = df.select(\"author\") \\\n",
    "                  .dropna(subset=[\"author\"]) \\\n",
    "                  .dropDuplicates([\"author\"])\n",
    "\n",
    "    if os.path.exists(user_path) and not is_folder_empty_of_parquet(user_path):\n",
    "        existing = spark.read.parquet(user_path).dropDuplicates([\"author\"])\n",
    "\n",
    "        # Get max user_id\n",
    "        max_id = existing.agg(spark_max(\"user_id\")).collect()[0][0] or 0\n",
    "\n",
    "        # Exclude existing authors\n",
    "        new_only = new_users.join(existing, on=\"author\", how=\"left_anti\")\n",
    "\n",
    "        if new_only.count() == 0:\n",
    "            return existing\n",
    "\n",
    "        # Assign new user_ids sequentially\n",
    "        window = Window.orderBy(\"author\")\n",
    "        new_only = new_only.withColumn(\"row_num\", dense_rank().over(window))\n",
    "        new_only = new_only.withColumn(\"user_id\", col(\"row_num\") + lit(max_id)).drop(\"row_num\")\n",
    "\n",
    "        # Combine and ensure uniqueness\n",
    "        combined = existing.unionByName(new_only).dropDuplicates([\"author\"])\n",
    "\n",
    "        # Safety check\n",
    "        assert combined.select(\"user_id\").distinct().count() == combined.count(), \"âŒ Duplicate user_id found!\"\n",
    "\n",
    "        # Overwrite clean version\n",
    "        shutil.rmtree(user_path)\n",
    "        combined.write.mode(\"overwrite\").parquet(user_path)\n",
    "        return combined\n",
    "\n",
    "    else:\n",
    "        # First-time creation\n",
    "        window = Window.orderBy(\"author\")\n",
    "        new_users = new_users.withColumn(\"user_id\", dense_rank().over(window))\n",
    "                \n",
    "        new_users.write.mode(\"overwrite\").parquet(user_path)\n",
    "        return new_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f0f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dim_subreddit(df, dw_path):\n",
    "    from pyspark.sql.functions import col, lit, dense_rank, max as spark_max\n",
    "    from pyspark.sql.window import Window\n",
    "\n",
    "    spark.catalog.clearCache()\n",
    "    sub_path = os.path.join(dw_path, \"dim_subreddit\")\n",
    "\n",
    "    # Clean new input: keep only unique subreddit_id\n",
    "    new_subs = df.select(\"subreddit\", \"subreddit_id\") \\\n",
    "                 .dropna(subset=[\"subreddit_id\"]) \\\n",
    "                 .dropDuplicates([\"subreddit_id\"])\n",
    "\n",
    "    if os.path.exists(sub_path) and not is_folder_empty_of_parquet(sub_path):\n",
    "        existing = spark.read.parquet(sub_path).dropDuplicates([\"subreddit_id\"])\n",
    "\n",
    "        # Get max existing subreddit_key\n",
    "        max_key = existing.agg(spark_max(\"subreddit_key\")).collect()[0][0] or 0\n",
    "\n",
    "        # Exclude already seen subreddit_ids\n",
    "        new_only = new_subs.join(existing, on=\"subreddit_id\", how=\"left_anti\")\n",
    "\n",
    "        if new_only.count() == 0:\n",
    "            return existing\n",
    "\n",
    "        # Assign subreddit_keys sequentially from max_key + 1\n",
    "        window = Window.orderBy(\"subreddit_id\")\n",
    "        new_only = new_only.withColumn(\"row_num\", dense_rank().over(window))\n",
    "        new_only = new_only.withColumn(\"subreddit_key\", col(\"row_num\") + lit(max_key)).drop(\"row_num\")\n",
    "\n",
    "        # Combine and drop any accidental duplicates\n",
    "        combined = existing.unionByName(new_only).dropDuplicates([\"subreddit_id\"])\n",
    "\n",
    "        # Final safety: ensure subreddit_key is unique\n",
    "        assert combined.select(\"subreddit_key\").distinct().count() == combined.count(), \"âŒ Duplicate subreddit_key found!\"\n",
    "\n",
    "        # Overwrite clean version\n",
    "        shutil.rmtree(sub_path)\n",
    "        \n",
    "        # ğŸš¨ Deduplicate by subreddit_key\n",
    "        combined = combined.dropDuplicates([\"subreddit_key\"])\n",
    "        \n",
    "        combined.write.mode(\"overwrite\").parquet(sub_path)\n",
    "        return combined\n",
    "\n",
    "    else:\n",
    "        # Initial load: assign keys from 1\n",
    "        window = Window.orderBy(\"subreddit_id\")\n",
    "        new_subs = new_subs.withColumn(\"subreddit_key\", dense_rank().over(window))\n",
    "        \n",
    "        # ğŸš¨ Deduplicate by subreddit_key\n",
    "        new_subs = new_subs.dropDuplicates([\"subreddit_key\"])\n",
    "        \n",
    "        new_subs.write.mode(\"overwrite\").parquet(sub_path)\n",
    "        return new_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d7a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dim_date(df, dw_path):\n",
    "    # Extract and cast timestamp\n",
    "    dim_date_df = df.withColumn(\"created_datetime\", from_unixtime(col(\"created_utc\")).cast(\"timestamp\")) \\\n",
    "                    .select(\"created_datetime\") \\\n",
    "                    .dropDuplicates()\n",
    "\n",
    "    # Breakdown into date parts\n",
    "    dim_date_df = dim_date_df \\\n",
    "        .withColumn(\"date\", to_date(col(\"created_datetime\"))) \\\n",
    "        .withColumn(\"year\", year(col(\"created_datetime\"))) \\\n",
    "        .withColumn(\"month\", month(col(\"created_datetime\"))) \\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"created_datetime\"))) \\\n",
    "        .withColumn(\"hour\", hour(col(\"created_datetime\"))) \\\n",
    "        .withColumn(\"minute\", minute(col(\"created_datetime\"))) \\\n",
    "        .withColumn(\"second\", second(col(\"created_datetime\"))) \\\n",
    "        .withColumn(\"day_of_week\", date_format(col(\"created_datetime\"), \"E\")) \\\n",
    "        .withColumn(\"week_of_year\", weekofyear(col(\"created_datetime\")))\n",
    "\n",
    "    # Path to save\n",
    "    date_path = os.path.join(dw_path, \"dim_date\")\n",
    "\n",
    "    # Append new entries only\n",
    "    if os.path.exists(date_path):\n",
    "        existing = spark.read.parquet(date_path)\n",
    "        dim_date_df = dim_date_df.join(existing, on=\"created_datetime\", how=\"left_anti\")\n",
    "    \n",
    "    # ğŸš¨ Deduplicate by created_datetime\n",
    "    dim_date_df = dim_date_df.dropDuplicates([\"created_datetime\"])\n",
    "    \n",
    "    dim_date_df.write.mode(\"append\").parquet(date_path)\n",
    "    return spark.read.parquet(date_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e7461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fact_comment(df, dim_user, dim_sub, dim_date, dw_path):\n",
    "    spark.catalog.clearCache()\n",
    "    fact_path = os.path.join(dw_path, \"fact_comment\")\n",
    "\n",
    "    # Step 1: Add timestamp for join\n",
    "    df = df.withColumn(\"created_datetime\", from_unixtime(col(\"created_utc\")).cast(\"timestamp\"))\n",
    "\n",
    "    # Step 2: Join with dimension tables\n",
    "    new_fact = df \\\n",
    "        .join(dim_user.select(\"author\", \"user_id\"), on=\"author\", how=\"left\") \\\n",
    "        .join(dim_sub.select(\"subreddit_id\", \"subreddit_key\"), on=\"subreddit_id\", how=\"left\") \\\n",
    "        .join(dim_date.select(\"created_datetime\"), on=\"created_datetime\", how=\"left\") \\\n",
    "        .select(\n",
    "            col(\"comment_id\"),\n",
    "            col(\"user_id\"),\n",
    "            col(\"subreddit_key\"),\n",
    "            col(\"created_datetime\"),\n",
    "            col(\"body\"),\n",
    "            col(\"score\"),\n",
    "            col(\"controversiality\"),\n",
    "            col(\"gilded\"),\n",
    "            col(\"ups\"),\n",
    "            col(\"downs\")\n",
    "        ).dropna(subset=[\"comment_id\"])  # Ensure we don't propagate null comment_ids\n",
    "\n",
    "    # Step 3: Load existing fact_comment if exists\n",
    "    if os.path.exists(fact_path):\n",
    "        existing_fact = spark.read.parquet(fact_path).dropDuplicates([\"comment_id\"])\n",
    "        # Keep only new records that don't exist\n",
    "        new_only = new_fact.join(existing_fact, on=\"comment_id\", how=\"left_anti\")\n",
    "        combined = existing_fact.unionByName(new_only).dropDuplicates([\"comment_id\"])\n",
    "        \n",
    "        # Filter invalid dates\n",
    "        combined = combined.filter(\n",
    "            (col(\"created_datetime\").isNotNull()) &\n",
    "            (to_date(\"created_datetime\") != lit(\"1969-12-31\"))\n",
    "        )\n",
    "        \n",
    "        # Print distinct dates\n",
    "        print(\"ğŸ—“ï¸ Distinct created dates being written:\")\n",
    "        combined.select(to_date(\"created_datetime\").alias(\"date\")).distinct().orderBy(\"date\").show(truncate=False)\n",
    "\n",
    "        # Remove old and overwrite\n",
    "        #shutil.rmtree(fact_path)\n",
    "        combined.write.mode(\"append\").parquet(fact_path)\n",
    "        return combined\n",
    "    else:\n",
    "        \n",
    "        # Filter invalid dates\n",
    "        new_fact = new_fact.filter(\n",
    "            (col(\"created_datetime\").isNotNull()) &\n",
    "            (to_date(\"created_datetime\") != lit(\"1969-12-31\"))\n",
    "        )\n",
    "        \n",
    "        # First time creation\n",
    "        new_fact.write.mode(\"overwrite\").parquet(fact_path)\n",
    "        return new_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e987719",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-01.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-02.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-03.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-04.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-05.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-06.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-07.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-08.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-09.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-10.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-11.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-12.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-13.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-14.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-15.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-16.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-17.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-18.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-19.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-20.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-21.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-22.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-23.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-24.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-25.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-26.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-27.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-28.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-29.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-30.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ğŸšš Processing file: D:/Portfolio/reddit-analytics-pipeline/data/partitions\\reddit_data_2015-05-31.csv\n",
      "ğŸ—“ï¸ Distinct created dates being written:\n",
      "+----------+\n",
      "|date      |\n",
      "+----------+\n",
      "|2015-05-01|\n",
      "|2015-05-02|\n",
      "|2015-05-03|\n",
      "|2015-05-04|\n",
      "|2015-05-05|\n",
      "|2015-05-06|\n",
      "|2015-05-07|\n",
      "|2015-05-08|\n",
      "|2015-05-09|\n",
      "|2015-05-10|\n",
      "|2015-05-11|\n",
      "|2015-05-12|\n",
      "|2015-05-13|\n",
      "|2015-05-14|\n",
      "|2015-05-15|\n",
      "|2015-05-16|\n",
      "|2015-05-17|\n",
      "|2015-05-18|\n",
      "|2015-05-19|\n",
      "|2015-05-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "âœ… ETL completed. All dimension and fact tables updated.\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ Main\n",
    "if __name__ == \"__main__\":\n",
    "    all_csvs = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "    for csv_file in all_csvs:\n",
    "        print(f\"ğŸšš Processing file: {csv_file}\")\n",
    "        df_raw = read_new_csvs(csv_file)\n",
    "        df_clean = clean_and_prepare(df_raw)\n",
    "\n",
    "\n",
    "        dim_user = build_dim_user(df_clean, dw_folder)\n",
    "        dim_sub = build_dim_subreddit(df_clean, dw_folder)\n",
    "        dim_date = build_dim_date(df_clean, dw_folder)\n",
    "        #dim_user = spark.read.parquet(os.path.join(dw_folder, \"dim_user\"))\n",
    "        #dim_sub = spark.read.parquet(os.path.join(dw_folder, \"dim_subreddit\"))\n",
    "        #dim_date = spark.read.parquet(os.path.join(dw_folder, \"dim_date\"))\n",
    "\n",
    "        fact_comment = build_fact_comment(df_clean, dim_user, dim_sub, dim_date, dw_folder)\n",
    "\n",
    "    print(\"âœ… ETL completed. All dimension and fact tables updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83bed263",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce2e87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
