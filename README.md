# Reddit Analytics ETL Pipeline

An end-to-end PySpark ETL pipeline that processes Reddit comment data from CSV files into structured Parquet data warehouse tables. It supports downstream analytics and dashboards using clean, de-duplicated, and optimized datasets.

---

## ğŸ”§ Features

- Incremental dimension updates to avoid duplicate keys.
- Automatic surrogate key assignment for dimension tables.
- Fact table joins to dimensions via clean foreign keys.
- Timestamp handling: UNIX to UTC timestamp conversion.
- Filters out malformed timestamps (e.g. null or 1969-12-31).
- Logs processed dates for monitoring.
- Duplicate prevention via primary key logic.

---

## ğŸ—‚ï¸ Data Source

This project uses public Reddit comment data from May 2015, available on Kaggle:

ğŸ”— Reddit Comments - May 2015 Dataset (https://www.kaggle.com/datasets/kaggle/reddit-comments-may-2015)

- Over 160 million Reddit comments
- Provided as a single sqlite file
- Source: Pushshift Reddit dataset via Kaggle
---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ partitions/                 # Input CSV files
â”‚   â””â”€â”€ DW_cache/                   # Output Parquet files (dimension + fact)
â”‚   â””â”€â”€ Raw/                        # Containg Raw SQLite Databse file.
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ fetch_reddit_data.ipynb     # Fetching data from sqlite databse file into csvs
â”‚   â””â”€â”€ ingest_and_transform.ipynb  # Transform the Reddit comments raw csvs into dimension and fact table parquet files
â”‚   â””â”€â”€ load_dim_date.ipynb         # Load parquet date dimension file into snowflake
â”‚   â””â”€â”€ load_dim_users.ipynb        # Load parquet user dimension files into snowflake
â”‚   â””â”€â”€ load_dim_subreddit.ipynb    # Load parquet subreddit dimension files into snowflake
â”‚   â””â”€â”€ load_fact_comments.ipynb    # Load parquet fact comment files into snowflake
â”œâ”€â”€ scripts/               # Contain similar files to load data into AWS redshift
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

```

---

## ğŸš€ Setup & Execution

### Prerequisites
- Python 3.8+
- PySpark


## ğŸ“… Tables Created

### Dimension Tables

- **dim_user**: Unique authors with surrogate `user_id`
- **dim_subreddit**: Unique subreddits with `subreddit_key`
- **dim_date**: Distinct dates from `created_datetime`

### Fact Table

- **fact_comment**:
  - `comment_id`
  - Foreign keys to `user_id`, `subreddit_key`, `date_key`
  - Metadata fields: score, ups, downs, body, etc.
  - Duplicates removed via `dropDuplicates(["comment_id"])`

---

## ğŸ“Š Logs & Debugging

During processing, each file:
- Prints distinct `created_datetime` dates being written.
- Skips malformed or empty files.
- Logs if parquet folders already exist.

Example:
```
ğŸšš Processing file: reddit_data_2015-05-01.csv
ğŸ—“ï¸ Distinct created dates being written:
|2015-05-01|
```

---

## ğŸ“„ Enhancements

- Add schema validation or automated tests.
- Replace overwrite with incremental write logic for fact.
- Add support for large datasets (partitioned writes).
- Add Spark job scheduling and parameter parsing.

---

## ğŸ¤ Contributing

Open to contributions for enhancements, performance improvements, or integration with BI tools.

---

Built with â¤ï¸ by [Ali-Shan-Khawaja](https://github.com/Ali-Shan-Khawaja)

